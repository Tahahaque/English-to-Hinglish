# -*- coding: utf-8 -*-
"""Hinglish.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UnfncHUF-_ZUJ-OFLdTSsVll-r7z4r8S
"""

!pip install transformers

!pip install datasets
!pip install accelerate==0.20.3
import datasets
from transformers import AutoConfig, AutoTokenizer, AutoModelForSeq2SeqLM
from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments
from transformers import HfArgumentParser
from transformers import default_data_collator

### MODEL ###
from transformers import AutoConfig, AutoModel,AutoTokenizer,AutoModelForSeq2SeqLM
model_name = "t5-small"
config = AutoConfig.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    use_fast=True, #important
    )
model = AutoModelForSeq2SeqLM.from_pretrained(
    model_name,
    config=config)

# transformers lib will fetch the config,
# tokenizer and pre-trained model as per the model_name
# tokenizer:  T5TokenizerFast, fast tokenizer can be trained for old ones
# model : T5ForConditionalGeneration

### DATA TOKENIZER ###
dataset = datasets.load_dataset('findnitai/english-to-hinglish')

# data set format will be json lines, two lines below as example
# {"translation" : {"en" : "Hello, what is your name?" , "hi_ng" : "Namaste aapka naam kya hai?" }}

### TOKENIZER ###
master = []
for line in dataset['train']['translation']:
    master.append(line['en'])
    master.append(line['hi_ng'])

def gen_training_data():
    return (master[i : i+500]
    for i in range(0, len(master), 500)
    )
tokenizer_training_data = gen_training_data()
tokenizer = tokenizer.train_new_from_iterator(tokenizer_training_data, 32128)
# tokenizer.save_pretrained("en-hig-tokenizer")

from google.colab import files
uploaded = files.upload()

# Load and preprocess the training data
train_file = "hinglish_upload_v1.json"

data_files = {}
data_files["train"] = train_file

raw_datasets = datasets.load_dataset("json", data_files=data_files)

source_prefix = "Translate English to Hinglish : "
source_lang = "en"
target_lang = "hi_ng"
max_source_length = 128
max_target_length = 128
padding = "max_length"
num_epochs = 1

def preprocess(source_data):
    inputs = [sample[source_lang] for sample in source_data["translation"]]
    targets = [sample[target_lang] for sample in source_data["translation"]]
    inputs = [source_prefix + inp for inp in inputs]
    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)
    labels = tokenizer(targets, max_length=max_target_length, padding=padding, truncation=True)
    labels["input_ids"] = [
        [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels["input_ids"]
    ]
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

train_dataset = raw_datasets["train"]
train_dataset = train_dataset.map(preprocess, batched=True, remove_columns="translation")
data_collator = default_data_collator


trainer_args_in = {
    'overwrite_output_dir' : True,
    'do_train' : True,
    # 'do_valid' : False,
    'per_device_train_batch_size' : 32,
    'num_train_epochs' : num_epochs,
}

#all arguments can be found detailed in the TrainingArguments dataclass

parser = HfArgumentParser((Seq2SeqTrainingArguments))
training_args = parser.parse_dict(trainer_args_in)

trainer = Seq2SeqTrainer(model=model, args=training_args[0], train_dataset=train_dataset, tokenizer=tokenizer, data_collator=data_collator)

train_result = trainer.train(resume_from_checkpoint=None)
trainer.save_model()

import torch

# Check if GPU is available, and set the device accordingly
device = "cuda:0" if torch.cuda.is_available() else "cpu"

# Your input text and tokenization code
input_text = "translate English to Hinglish:I was waiting for my bag"
input_ids = tokenizer(input_text, return_tensors="pt").input_ids
input_ids = input_ids.to(device)  # Move input tensor to the selected device

# Generate the output
outputs = model.generate(input_ids)

# Decode and print the output
print("Test Output: " + tokenizer.decode(outputs[0], skip_special_tokens=True))

input_text = "translate English to Hinglish: I was waiting for my bag"
input_ids = tokenizer(input_text, return_tensors="pt").input_ids
outputs = model.generate(input_ids)
translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

print("Test Input: " + input_text)
print("Test Output: " + translated_text)
